# -*- coding: utf-8 -*-
"""Recommendation  system with video generation using multi model approach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z_5ZNb_LexOC1uiYXQf1AJ-cPhWQ3I3r

# Ateema Capstone Project

## Local models

#### Embedding

[GPT4All Embeddings](https://blog.nomic.ai/posts/nomic-embed-text-v1):

```
pip install langchain-nomic
```

### LLM

Use [Ollama](https://ollama.ai/) and [llama3](https://ollama.ai/library/llama3):

```
ollama pull llama3
```

Prompt -

https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/

## Libraries
"""

!pip install gpt4all

!pip install pyngrok

!curl https://ollama.ai/install.sh | sh

!pip install langchain-nomic

!pip install langchain

!pip install -U langchain-community

!pip install langgraph

!pip install httpx

!pip install tiktoken

!pip install chromadb

!pip install accelerate

!pip install --upgrade transformers diffusers

!pip install diffusers==0.10.2

!pip install transformers==4.19.2

# Commented out IPython magic to ensure Python compatibility.
# the scipy version packaged with colab is not tolerant of misformated WAV files.
# install the latest version.
!pip3 install -U scipy

!git clone https://github.com/jnordberg/tortoise-tts.git
# %cd tortoise-tts
!pip3 install transformers==4.19.0
!pip3 install -r requirements.txt
!python3 setup.py install

# Commented out IPython magic to ensure Python compatibility.
# the scipy version packaged with colab is not tolerant of misformated WAV files.
# install the latest version.
!pip3 install -U scipy

!git clone https://github.com/jnordberg/tortoise-tts.git
# %cd tortoise-tts
!pip3 install transformers==4.19.0
!pip3 install -r requirements.txt
!python3 setup.py install

"""## Setup"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os

from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders.csv_loader import CSVLoader
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain.schema import Document
from langgraph.graph import END, StateGraph

from typing_extensions import TypedDict
from typing import List
from pprint import pprint

# LLM
local_llm = 'llama3'

import os
from pyngrok import ngrok

# replace with actual ngrok authtoken
authtoken = ""

ngrok.set_auth_token(authtoken)

# Start the Ollama server
def start_ollama_server():
    os.system("ollama serve")

# Create and start a thread to run the Ollama server
import threading
import time

ollama_thread = threading.Thread(target=start_ollama_server)
ollama_thread.start()

# Wait a few seconds to ensure the server has started
time.sleep(10)

# Create a tunnel to the Ollama server
ngrok_tunnel = ngrok.connect(11434)
print(f"ngrok tunnel opened at: {ngrok_tunnel.public_url}")

from pyngrok import ngrok

# List existing tunnels
tunnels = ngrok.get_tunnels()

# Print existing tunnels
print("Existing Tunnels:")
for tunnel in tunnels:
    print(tunnel.public_url)

import threading
from pyngrok import ngrok

def ollama():
    !ollama serve

ollama_thread = threading.Thread(target=ollama)
ollama_thread.start()

# data loader
loader = CSVLoader(file_path="/content/drive/MyDrive/stage1_all.csv")
data = loader.load()

from gpt4all import Embed4All
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# Data transformers
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=250, chunk_overlap=0
)
texts = text_splitter.split_documents(data)

# Initialize embeddings
embedder = Embed4All()

# Generate embeddings for documents
embeddings = [embedder.embed(doc.page_content) for doc in texts]

# Create a dictionary to map document content to their embeddings
embedding_dict = {doc.page_content: embedding for doc, embedding in zip(texts, embeddings)}

# Define a custom embedding function
class CustomEmbeddingFunction:
    def __init__(self, embedder, embedding_dict):
        self.embedder = embedder
        self.embedding_dict = embedding_dict

    def embed_documents(self, texts):
        return [self.embedding_dict[text] for text in texts]

    def embed_query(self, text):
        return self.embedder.embed(text)

# Initialize the custom embedding function
custom_embedding = CustomEmbeddingFunction(embedder, embedding_dict)

# Add to vectorDB
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=custom_embedding,
    collection_name="rag-chroma"
)

# Retriever
retriever = vectorstore.as_retriever()

!ollama pull llama3

"""## Retrieval Grader

"""

# LLM
llm = ChatOllama(model="llama3", format="json", temperature=0)

# Define prompt
prompt = PromptTemplate(
    template="""system You are a grader assessing relevance
    of a retrieved document to a user question. If the document contains keywords related to the user question,
    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n
    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.
     user
    Here is the retrieved document: \n\n {document} \n\n
    Here is the user question: {question} \n assistant
    """,
    input_variables=["question", "document"],
)

retrieval_grader = prompt | llm | JsonOutputParser()

# Example question
question = "My name is Sanjay. I will be visiting Chicago in summer with my family to explore and would like to go to interesting places."

# Retrieve documents
docs = retriever.get_relevant_documents(question)
doc_txt = docs[0].page_content

# Grade the relevance
result = retrieval_grader.invoke({"question": question, "document": doc_txt})
print(result)

"""## Generation

"""

# # Prompt
# prompt = PromptTemplate(
#     template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.
#     Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.
#     Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>
#     Question: {question}
#     Context: {context}
#     Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
#     input_variables=["question", "document"],
# )

prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI tour guide named "Not the real Barack Obama, the president, former governor of Chicago". Your role is to assist users in finding interesting places to visit.
    Greet the user warmly and provide recommendations based on their specified location. Provide a detailed description of each place ranked up to 10 places and why it might be interesting to the user.
    The prompt length targets up to 60 seconds that explicitly states what I asked you to and end a prompt like a professional tour guide as there will be no continuging conversation after generating a prompt. <|eot_id|><|start_header_id|>user<|end_header_id|>
    Question: {question}
    Context: {context}
    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["question", "document"],
)

llm = ChatOllama(model=local_llm, temperature=0)

# Post-processing
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Chain
rag_chain = prompt | llm | StrOutputParser()


# Run
question = "My name is Sanjay. I will be visiting Chicago in summer with my family to explore and would like to go to interesting places."

# Understanding retriever relevant metrics
docs = retriever.invoke(question)
generation = rag_chain.invoke({"context": docs, "question": question})
print(generation)

"""## Hallucination Grader

"""

# LLM
llm = ChatOllama(model=local_llm, format="json", temperature=0)

# Prompt
prompt = PromptTemplate(
    template=""" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether
    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate
    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a
    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here are the facts:
    \n ------- \n
    {documents}
    \n ------- \n
    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["generation", "documents"],
)

hallucination_grader = prompt | llm | JsonOutputParser()
hallucination_grader.invoke({"documents": docs, "generation": generation})

"""## Answer Grader

"""

# LLM
llm = ChatOllama(model=local_llm, format="json", temperature=0)

# Prompt
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an
    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is
    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.
     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:
    \n ------- \n
    {generation}
    \n ------- \n
    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["generation", "question"],
)

answer_grader = prompt | llm | JsonOutputParser()
answer_grader.invoke({"question": question,"generation": generation})

"""## Control Flow in LangGraph

"""

### State

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents
    """
    question : str
    generation : str
    documents : List[str]

### Nodes

def retrieve(state):
    """
    Retrieve documents from vectorstore

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]

    # Retrieval
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question}

def generate(state):
    """
    Generate answer using RAG on retrieved documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]

    # RAG generation
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"documents": documents, "question": question, "generation": generation}

def grade_documents(state):
    """
    Determines whether the retrieved documents are relevant to the question

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Filtered out irrelevant documents
    """

    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    # Score each doc
    filtered_docs = []
    for d in documents:
        score = retrieval_grader.invoke({"question": question, "document": d.page_content})
        grade = score['score']
        # Document relevant
        if grade.lower() == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        # Document not relevant
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            continue
    return {"documents": filtered_docs, "question": question}


def route_question(state):
    """
    Route question to web search or RAG.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    question = state["question"]
    print(question)


def decide_to_generate(state):
    """
    Determines whether to generate an answer

    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """

    print("---ASSESS GRADED DOCUMENTS---")
    question = state["question"]
    filtered_documents = state["documents"]

    # We have relevant documents, so generate answer
    print("---DECISION: GENERATE---")
    return "generate"

def grade_generation_v_documents_and_question(state):
    """
    Determines whether the generation is grounded in the document and answers question.

    Args:
        state (dict): The current graph state

    Returns:
        str: Decision for next node to call
    """

    print("---CHECK HALLUCINATIONS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    score = hallucination_grader.invoke({"documents": documents, "generation": generation})
    grade = score['score']

    # Check hallucination
    if grade == "yes":
        print("---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---")
        # Check question-answering
        print("---GRADE GENERATION vs QUESTION---")
        score = answer_grader.invoke({"question": question,"generation": generation})
        grade = score['score']
        if grade == "yes":
            print("---DECISION: GENERATION ADDRESSES QUESTION---")
            return "useful"
        else:
            print("---DECISION: GENERATION DOES NOT ADDRESS QUESTION---")
            return "not useful"
    else:
        pprint("---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---")
        return "not supported"

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("route_question", route_question) # route question
workflow.add_node("retrieve", retrieve) # retrieve
workflow.add_node("grade_documents", grade_documents) # grade documents
workflow.add_node("generate", generate) # generate

"""## Build Graph"""

# Build graph
workflow.set_entry_point("route_question")
workflow.add_edge("route_question", "retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "generate": "generate",
    },
)
workflow.add_conditional_edges(
    "generate",
    grade_generation_v_documents_and_question,
    {
        "not supported": "generate",
        "useful": END,
        "not useful": "retrieve",
    },
)

"""## Compile & Test"""

# Compile
app = workflow.compile()

# Test
inputs = {"question": "My name is Sanjay. I will be visiting Chicago in summer with my family to explore and would like to go to interesting places."}
for output in app.stream(inputs):
    for key, value in output.items():
        pprint(f"Finished running: {key}:")
pprint(value["generation"])

"""#SadTalker Avatar video generation + edge_tss text-to-audio-Setup"""

!pip install numpy scipy pillow opencv-python-headless scikit-image
!pip install gdown paramiko
!pip install kornia
!pip install yacs
!pip install gfpgan
!pip install pydub
!pip install facexlib
!pip install torchaudio==2.0.0
!pip install torch==2.0.0+cu117 torchvision==0.15.0+cu117 -f https://download.pytorch.org/whl/torch_stable.html
!apt-get update
!apt-get install ffmpeg

from google.colab import drive
drive.mount('/content/drive')

import os
base_dir = '/content/drive/MyDrive/SadTalker'
checkpoints_dir = os.path.join(base_dir, 'checkpoints')
weights_dir = os.path.join(base_dir, 'weights')

# Downloading the checkpoints to Google Drive
!gdown https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00109-model.pth.tar -O {checkpoints_dir}/mapping_00109-model.pth.tar
!gdown https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00229-model.pth.tar -O {checkpoints_dir}/mapping_00229-model.pth.tar
!gdown https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_256.safetensors -O {checkpoints_dir}/SadTalker_V0.0.2_256.safetensors
!gdown https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_512.safetensors -O {checkpoints_dir}/SadTalker_V0.0.2_512.safetensors

# Downloading facexlib and GFPGAN weights to Google Drive
!gdown https://github.com/xinntao/facexlib/releases/download/v0.1.0/alignment_WFLW_4HG.pth -O {weights_dir}/alignment_WFLW_4HG.pth
!gdown https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth -O {weights_dir}/detection_Resnet50_Final.pth
!gdown https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth -O {weights_dir}/GFPGANv1.4.pth
!gdown https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth -O {weights_dir}/parsing_parsenet.pth
!gdown https://github.com/Winfredy/SadTalker/releases/download/v0.0.2/epoch_20.pth -O {checkpoints_dir}/epoch_20.pth

# Download the checkpoints to Google Drive if they don't already exist
def download_if_not_exists(url, dest):
    if not os.path.exists(dest):
        !gdown {url} -O {dest}

download_if_not_exists('https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00109-model.pth.tar', f'{checkpoints_dir}/mapping_00109-model.pth.tar')
download_if_not_exists('https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/mapping_00229-model.pth.tar', f'{checkpoints_dir}/mapping_00229-model.pth.tar')
download_if_not_exists('https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_256.safetensors', f'{checkpoints_dir}/SadTalker_V0.0.2_256.safetensors')
download_if_not_exists('https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_512.safetensors', f'{checkpoints_dir}/SadTalker_V0.0.2_512.safetensors')

download_if_not_exists('https://github.com/xinntao/facexlib/releases/download/v0.1.0/alignment_WFLW_4HG.pth', f'{weights_dir}/alignment_WFLW_4HG.pth')
download_if_not_exists('https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth', f'{weights_dir}/detection_Resnet50_Final.pth')
download_if_not_exists('https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth', f'{weights_dir}/GFPGANv1.4.pth')
download_if_not_exists('https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth', f'{weights_dir}/parsing_parsenet.pth')
download_if_not_exists('https://github.com/Winfredy/SadTalker/releases/download/v0.0.2/epoch_20.pth', f'{checkpoints_dir}/epoch_20.pth')

!pip install edge-tts

"""# Text to Audio model TTS

**Text to Audio charaector training**
"""

!pip install pytube3

# Install necessary packages and setup (if not already done)
!pip install pydub
!apt-get install ffmpeg -y

from moviepy.editor import VideoFileClip
import os
import torchaudio

# video file of the chararecter
video_file = '/content/drive/MyDrive/YoutubeVid/barack.mp4'

# loading the video clip
clip = VideoFileClip(video_file)

# extracting audio from the first 35 seconds
audio_clip = clip.audio.subclip(0, 35)

# the directory where you want to save the clips
save_directory = '/content/drive/MyDrive/tortoise-tts/tortoise/voices/obama_voice'

# the directory
os.makedirs(save_directory, exist_ok=True)

# sacving the audio clips in WAV format
clip1_wav = os.path.join(save_directory, '1.wav')
clip2_wav = os.path.join(save_directory, '2.wav')
clip3_wav = os.path.join(save_directory, '3.wav')

# extracting and saving the first 15 seconds
audio_clip1 = audio_clip.subclip(0, 15)
audio_clip1.write_audiofile(clip1_wav, fps=22050, nbytes=4, codec='pcm_f32le')

# extracting and saving the next 15 seconds
audio_clip2 = audio_clip.subclip(15, 30)
audio_clip2.write_audiofile(clip2_wav, fps=22050, nbytes=4, codec='pcm_f32le')

# extracting and saving the remaining seconds up to 35 seconds (15 seconds clip)
audio_clip3 = audio_clip.subclip(30, 45)
audio_clip3.write_audiofile(clip3_wav, fps=22050, nbytes=4, codec='pcm_f32le')

# verifying the properties of the saved WAV files
print(torchaudio.info(clip1_wav))
print(torchaudio.info(clip2_wav))
print(torchaudio.info(clip3_wav))

# Checking the duration of each clip
def get_audio_duration(filepath):
    info = torchaudio.info(filepath)
    duration = info.num_frames / info.sample_rate
    return duration

clip1_duration = get_audio_duration(clip1_wav)
clip2_duration = get_audio_duration(clip2_wav)
clip3_duration = get_audio_duration(clip3_wav)

print(f"Duration of {clip1_wav}: {clip1_duration} seconds")
print(f"Duration of {clip2_wav}: {clip2_duration} seconds")
print(f"Duration of {clip3_wav}: {clip3_duration} seconds")

"""**Text to Audio converting using tortoise library**"""

import re
import asyncio
import nest_asyncio
from pydub import AudioSegment

import torch
import torchaudio
import torch.nn as nn
import torch.nn.functional as F
import shutil
import tortoise.api

import IPython

from tortoise.api import TextToSpeech
from tortoise.utils.audio import load_audio, load_voice
tts = TextToSpeech()

import os
import sys
import re
import asyncio
import nest_asyncio
from pydub import AudioSegment
import torch
import torchaudio
import IPython
from tortoise.api import TextToSpeech
from tortoise.utils.audio import load_voice, get_voices

#  path to the Tortoise-TTS directory
tortoise_tts_path = '/content/drive/MyDrive/tortoise-tts/'
sys.path.append(tortoise_tts_path)
os.chdir(tortoise_tts_path)

# necessary packages and setup (if not already done)
nest_asyncio.apply()

# function to clean text
def clean_text(text):
    text = re.sub(r'\*+', '', text)  # Remove asterisks
    text = re.sub(r'\s+', ' ', text)  # Remove excessive whitespace
    text = text.strip()  # Strip leading and trailing whitespace
    text = text.replace('\n', ' ')  # Replace newlines with spaces
    return text

# function to split text into chunks of a specified max length
def split_text(text, max_length=400):
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 > max_length:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += " " + sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# function to convert text to audio using Tortoise-TTS
def text_to_audio_tortoise(text, filename, voice):
    tts = TextToSpeech()
    preset = "high_quality"

    # the directory where voices are stored
    extra_voice_dirs = ['/content/drive/MyDrive/tortoise-tts/tortoise/voices/']

    # loading the 'obama_voice' using the specified directory
    voice_samples, conditioning_latents = load_voice(voice, extra_voice_dirs)

    # checking if the voice is loaded successfully
    if voice_samples is not None:
        print("Voice loaded successfully!")
        # spliting the text into chunks
        text_chunks = split_text(text)

        # generating speech for each chunk and concatenate the results
        combined_audio = AudioSegment.empty()
        for idx, chunk in enumerate(text_chunks):
            gen = tts.tts_with_preset(chunk, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)
            temp_filename = f"temp_audio_{idx}.wav"
            torchaudio.save(temp_filename, gen.squeeze(0).cpu(), 2400)
            audio_segment = AudioSegment.from_wav(temp_filename)
            combined_audio += audio_segment

        # saving the combined audio to the specified filename
        combined_audio.export(filename, format="wav")
        IPython.display.display(IPython.display.Audio(filename))
    else:
        print("Failed to load voice.")

generated_text = value["generation"]

# the cleasning the LLM-generated text
clean_generated_text = clean_text(generated_text)

# the path for the generated audio file
audio_filename = "/content/drive/MyDrive/SadTalker/examples/driven_audio/generated_audio.wav"

# the desired voice (replace with any desired voice voice name)
voice_name = 'obama_voice'

# converting cleaned text to audio with Tortoise-TTS
text_to_audio_tortoise(clean_generated_text, audio_filename, voice_name)

print(f"Audio file saved as {audio_filename}")

"""Old but quick way of converting text to auddio using edge_tss"""

import re
import edge_tts
import asyncio
import nest_asyncio

# nested event loops
nest_asyncio.apply()

# function to clean text
def clean_text(text):
    # removing asterisks
    text = re.sub(r'\*+', '', text)
    # removing excessive whitespace
    text = re.sub(r'\s+', ' ', text)
    # removing leading and trailing whitespace
    text = text.strip()
    # replacing newlines with spaces
    text = text.replace('\n', ' ')
    return text

# function to convert text to audio
def text_to_audio(text, filename):
    async def _convert():
        communicator = edge_tts.Communicate(text, voice="en-US-GuyNeural")
        await communicator.save(filename)

    loop = asyncio.get_event_loop()
    loop.run_until_complete(_convert())

# cleaning the generated text
clean_generated_text = clean_text(value["generation"])

# setting the path for the generated audio file
audio_filename = "/content/drive/MyDrive/SadTalker/examples/driven_audio/generated_audio.wav"

# converting cleaned text to audio
text_to_audio(clean_generated_text, audio_filename)
print(f"Audio file saved as {audio_filename}")

"""#Avatar video generation using SadTalker"""

# SadTalker inference prep
source_image_path = "/content/drive/MyDrive/SadTalker/examples/source_image/obama.png"
result_dir = "/content/drive/MyDrive/SadTalker/results"
checkpoint_dir = "/content/drive/MyDrive/SadTalker/checkpoints"
audio_filename = "/content/drive/MyDrive/SadTalker/examples/driven_audio/generated_audio.wav"

# directories checking
os.makedirs(result_dir, exist_ok=True)
os.makedirs(checkpoint_dir, exist_ok=True)

#SadTalker inference
!python /content/drive/MyDrive/SadTalker/inference.py --driven_audio {audio_filename} --source_image {source_image_path} --result_dir {result_dir} --still --preprocess full --enhancer gfpgan --checkpoint_dir {checkpoint_dir} --batch_size 1

# ensuring we are getting the latest generated video
def get_latest_generated_video(result_dir):
    video_files = [os.path.join(result_dir, file) for file in os.listdir(result_dir) if file.endswith(".mp4")]
    if not video_files:
        return None
    latest_video = max(video_files, key=os.path.getmtime)
    return latest_video

generated_video = get_latest_generated_video(result_dir)
print(f"Generated Video: {generated_video}")

# Displaying the video in the notebook
if generated_video:
    from IPython.display import display, Video
    display(Video(generated_video, embed=True, width=600, height=400))
else:
    print("No video file generated.")

"""# Stable diffusion model (Text to Image Demo)

**generating background images using stableDiffusionm text to image model**

---
"""

from PIL import Image, ImageDraw, ImageFont
import torch
from diffusers import StableDiffusionPipeline
import re

text = value["generation"]

# patterns of generated text
pattern1 = r"\*\*Ranking\s#\d+:\s(.*?)\*\*|\*\*[\d]+\.\s(.*?):"
pattern2 = r"\d+\.\s+\*\*(.*?):"

# matching pattern1 first
matches = re.findall(pattern1, text)
if matches:
    places = [place.strip().replace("**", "") for match in matches for place in match if place.strip()]
else:
    # If pattern1 doesn't match, try pattern2
    matches = re.findall(pattern2, text)
    places = [place.strip().replace("**", "") for place in matches]

print("Extracted places:", places)

"""**STableDiffusion Text to Image**"""

# initializing the stablediffu model
device = "cuda" if torch.cuda.is_available() else "cpu"
model = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16).to(device)

# the function to generate an image and add a label
def generate_image_with_label(place):
    with torch.no_grad():
        image = model(f"{place} in Chicago").images[0]
    image = image.convert("RGB")

    # creating a draw object to add text
    draw = ImageDraw.Draw(image)

    # loading the chosen font
    try:
        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"  # the font's path
        font_size = 80
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        # made this to fallback to the default font if specific one cannot be loaded
        font = ImageFont.load_default()
        font_size = 40

    # calculating the text size and position
    max_width = image.width - 20  # the max width for the text with padding
    text_bbox = draw.textbbox((0, 0), place, font=font)
    text_width = text_bbox[2] - text_bbox[0]

    # adjusting the font size to fit the image width
    while text_width > max_width and font_size > 10:
        font_size -= 5
        font = ImageFont.truetype(font_path, font_size)
        text_bbox = draw.textbbox((0, 0), place, font=font)
        text_width = text_bbox[2] - text_bbox[0]

    text_height = text_bbox[3] - text_bbox[1]
    text_position = ((image.width - text_width) / 2, 20)  # I can ddjust this to place the text at the top with a margin

    # drawing a rectangle background for the text
    rectangle_padding = 10
    rect_position = [
        text_position[0] - rectangle_padding,
        text_position[1] - rectangle_padding,
        text_position[0] + text_width + rectangle_padding,
        text_position[1] + text_height + rectangle_padding
    ]
    draw.rectangle(rect_position, fill="black")

    # drawing the text on the image
    draw.text(text_position, place, font=font, fill="white")

    return image

# generating and saving the images
images = {}
for place in places:
    images[place] = generate_image_with_label(place)
    image_path = f"/content/drive/MyDrive/StableDiff_Images/{place.replace(' ', '_')}.png"
    images[place].save(image_path)

print("Images with labels generated for each place.")

"""converting audio file to calculate the video duration (img timestamping)"""

from pydub import AudioSegment

# Convert audio file to PCM WAV format
audio_path = "/content/drive/MyDrive/SadTalker/examples/driven_audio/generated_audio.wav"
converted_audio_path = "/content/drive/MyDrive/SadTalker/examples/driven_audio/converted_audio.wav"

audio = AudioSegment.from_file(audio_path)
audio.export(converted_audio_path, format="wav")

print(f"Audio file converted and saved as {converted_audio_path}")

"""timestamping images from both generated text and converted audio"""

import re
import moviepy.editor as mp

# generated text that is stored in the variable 'value["generation"]')
text = value["generation"]

# the path to the converted audio file
converted_audio_path = "/content/drive/MyDrive/SadTalker/examples/driven_audio/converted_audio.wav"

# calculating the duration of the audio clip
audio_clip = mp.AudioFileClip(converted_audio_path)
audio_duration = audio_clip.duration

# defineing patterns to extract places
pattern1 = r"\*\*Ranking\s#\d+:\s(.*?)\*\*|\*\*[\d]+\.\s(.*?):"
pattern2 = r"\d+\.\s+\*\*(.*?):"

# first trying to matching pattern1 first
matches = re.findall(pattern1, text)
if matches:
    places = [place.strip().replace("**", "") for match in matches for place in match if place.strip()]
else:
    # If pattern1 doesn't match, try pattern2
    matches = re.findall(pattern2, text)
    places = [place.strip().replace("**", "") for place in matches]

# caslculating the timestamps based on the text and audio duration
timestamps = {}
words = text.split()

# the funtion to find best start time based on matched words
def find_best_start_time(place_words):
    total_place_words = len(place_words)
    best_start_time = None
    best_match_count = 0

    # to iterate through words to find the best matching segment
    for i in range(len(words) - total_place_words + 1):
        match_count = sum(1 for j in range(total_place_words) if place_words[j].lower() in words[i + j].lower())

        if match_count > best_match_count:
            best_match_count = match_count
            # calulating start time based on the position of the first word
            start_index = i
            end_index = i + total_place_words
            best_start_time = ((start_index / len(words)) * audio_duration)  # adjusted by -10 seconds if needed

    return best_start_time

# calcuatingf timestamps for each place
for place in places:
    place_words = place.split()
    best_start_time = find_best_start_time(place_words)

    if best_start_time is not None:
        timestamps[place] = best_start_time

print("Timestamps:", timestamps)

"""video prodduction code"""

import moviepy.editor as mp
from PIL import Image

# loading the required files and directrories
audio_filename = "/content/drive/MyDrive/SadTalker/examples/driven_audio/generated_audio.wav"
converted_audio_path = "/content/drive/MyDrive/SadTalker/examples/driven_audio/converted_audio.wav"
result_dir = "/content/drive/MyDrive/SadTalker/results"
generated_video_path = get_latest_generated_video(result_dir)
initial_background_path = "/content/drive/MyDrive/StableDiff_Images/ateem_background.PNG"

# calculating the duration of the audio
audio_clip = mp.AudioFileClip(converted_audio_path)
audio_duration = audio_clip.duration

# converting images to RGB format using PIL
def convert_to_rgb(image_path):
    image = Image.open(image_path)
    if image.mode != 'RGB':
        image = image.convert('RGB')
    return mp.ImageClip(np.array(image))

# creating the video with synchronized images and speech
def create_synchronized_video(audio_path, image_paths, timestamps, output_path, generated_video_path, audio_duration, initial_background_path):
    # loading the avatar video and audio
    video_clip = mp.VideoFileClip(generated_video_path)
    audio_clip = mp.AudioFileClip(audio_path)
    final_clips = []

    # sorting thr timestamps
    sorted_timestamps = sorted(timestamps.items(), key=lambda x: x[1])

    # adding the initial background image until the first timestamp
    first_timestamp = sorted_timestamps[0][1]
    initial_clip = convert_to_rgb(initial_background_path).set_duration(first_timestamp)
    initial_clip = initial_clip.set_position(("center", "center")).resize(height=video_clip.h)
    initial_clip = initial_clip.crossfadein(1).crossfadeout(1)
    final_clips.append(initial_clip)

    # adding background images at the specified timestamps with fade effects
    for i, (place, start_time) in enumerate(sorted_timestamps):
        if place in image_paths:
            end_time = sorted_timestamps[i + 1][1] if i + 1 < len(sorted_timestamps) else audio_duration
            img_clip = convert_to_rgb(image_paths[place]).set_start(start_time).set_duration(end_time - start_time)
            img_clip = img_clip.set_position(("center", "center")).resize(height=video_clip.h)
            img_clip = img_clip.crossfadein(1).crossfadeout(1)
            final_clips.append(img_clip)

    # creating the clip with background images and transitions
    background_clip = mp.CompositeVideoClip(final_clips, size=(video_clip.w, video_clip.h))

    # rezing and positioning of the avatar video
    avatar_height = 200  # height to make the avatar video smaller or bigger
    avatar_clip = video_clip.resize(height=avatar_height).set_position(("right", "bottom"))

    # grouping all clips together
    final_clip = mp.CompositeVideoClip([background_clip, avatar_clip.set_start(0).set_duration(audio_duration)]).set_audio(audio_clip)
    final_clip.write_videofile(output_path, codec="libx264", audio_codec="aac")

# images
image_paths = {place: f"/content/drive/MyDrive/StableDiff_Images/{place.replace(' ', '_')}.png" for place in timestamps.keys()}

# path to save the final video
output_video_path = "/content/drive/MyDrive/StableDiff_Images/final_video.mp4"

# creating the the synchronized video
create_synchronized_video(audio_filename, image_paths, timestamps, output_video_path, generated_video_path, audio_duration, initial_background_path)

# displaying the final video in the notebook
from IPython.display import display, Video
display(Video(output_video_path, embed=True, width=600, height=400))